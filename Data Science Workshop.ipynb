{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# New York City Taxi Ride Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, wee will explore the New York City taxi trip records. The New York City Taxi and Limousine Commission (TLC) provides the data is open datasets. It was collected by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The TLC makes no warranties about the accuracy of this data.\n",
    "\n",
    "We will use the NYC taxi trip data from 2015 to predict the duration of a Taxi trip for a taxi ride in New York City given the pickup and dropoff locations and date & time. At your next visit to New York such that you at your next visit to New York can make an informed descion between public transport (e.g. provided by the [NYC Transit App](https://apps.apple.com/us/app/transit-bus-subway-times/id498151501) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://media.giphy.com/media/2ZXsHDuIhvu2yxMgFI/giphy.gif' align='center'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Loading and setting up our python environment\n",
    "***\n",
    "\n",
    "As a first step we load the python modules, that we will use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python modules for data visualisation. In addition, we set the layout for data visualisation to seaborn poster style. Just to make everything pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"poster\", font_scale=1.2)\n",
    "current_palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft Azure specific python modules and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(workspace=ws, name=\"New-York-City_Taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science development cycle\n",
    "***\n",
    "\n",
    "The development of machine learning solutions is highly iterative. The development cycle starts with getting the right data. In real applications this is the field where Internet of Things (IoT) technologies and Data Science/ Machine Learning enrich each other. For example, the TLC had to implement and roleout a solution that collects the information of every taxi ride in New York City. This solution had to be implemented in such a way that it is accepted by the drivers on the one hand side and scalable to at best all taxis registered with the TLC.\n",
    "Happily, this problem was solved and the resulting data is provided by the TLC open for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/DataScienceDevCycle_1.png' align='center' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from the [TLC Trip Record Data webpage](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "For the purpose of this tutorial, we already downloaded the data for you for further processing. Into the directory NYC_yellow_cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "datapath=os.path.join(cwd, \"NYC_yellow_cap\")\n",
    "datafiles = [os.path.join(datapath, f) for f in os.listdir(datapath) if f[-7:] == 'csv.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from the year 2015. The taxi trip records are stored in comma separated values (csv) files with one file per month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_list = [pd.read_csv(f, sep=',', engine='c') for f in datafiles]\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "***\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_2.png' align='center' width=\"500\"/>\n",
    "\n",
    "Before we explore the data in detail. We will have a look on the data records and what each column represents. An official description of the included record fields if given in the [pdf](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) on their webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are approx 18 million records in our dataset. However, the original dataset provided by the TLC for the year 2015 is 8 times bigger containing almost 150 million records.\n",
    "\n",
    "### Feature details:\n",
    "\n",
    "* passenger_count - the number of passengers in the vehicle (driver entered value)\n",
    "* pickup_datetime - date and time when the meter was engaged\n",
    "* dropoff_datetime - date and time when the meter was disengaged\n",
    "* pickup_longitude - the longitude where the meter was engaged\n",
    "* pickup_latitude - the latitude where the meter was engaged\n",
    "* dropoff_longitude - the longitude where the meter was disengaged\n",
    "* dropoff_latitude - the latitude where the meter was disengaged\n",
    "* tip_amount - This field is automatically populated for credit card tips. Cash tips are not included\n",
    "* total_amount - The total amount charged to passengers. Does not include cash tips (including tips, taxs, fares, surcharge)\n",
    "           \n",
    "### Label details:\n",
    "* trip_duration - duration of the trip in seconds\n",
    "\n",
    "### Computed details:\n",
    "* distance - was calculated in advance from the pick-up and drop-off coordinates using [Haversine formula](\n",
    "https://en.wikipedia.org/wiki/Haversine_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Taxi passenger count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "sns.countplot(data.passenger_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "Most of trip consist of passenger either 1 or 2.\n",
    "There are some trips with 0 passenger count.\n",
    "Few trips consisted of even 7, 8 or 9 passengers.\n",
    "\n",
    "#### Trips with 0 passenger:\n",
    "Passenger count is a driver entered value. Since the trip is not possible without passengers. It is evident that the driver forgot to enter the value for the trips with 0 passenger count. Lets analyze the passenger count distribution further to make it consistent for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = data.passenger_count.mean()\n",
    "median = data.passenger_count.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.boxplot(data.passenger_count, ax=ax)\n",
    "ax.axvline(mean, color='r', linestyle='--')\n",
    "ax.axvline(median, color='g', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea:\n",
    "As we see in the boxplot, the mean (red line) and median (green line) values are approx equal to 1. We therefore replace the 0 passenger count with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['passenger_count'] = data.passenger_count.map(lambda x: 1 if x == 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea:\n",
    "#### Trips with 7, 8 or 9 passenger:\n",
    "\n",
    "The New York City Taxi Passenger Limit says:\n",
    "* A maximum of 4 passengers can ride in traditional cabs\n",
    "* There 5 passenger cabs (minivans).\n",
    "* One child under 7 is allowed to sit on a passenger's lap in the rear seat in addition to the passenger limit.\n",
    "\n",
    "We can conclude, that in total a maximum of 6 passenger can board a new york taxi i.e. 5 adult + 1 child. Thus, we remove the records with passenger count > 7, 8 or 9 because they suspicious extreme values and may hint to inconsitent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.passenger_count <= 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is consistent with respect to the passenger count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of any unforseen issues load data and continue from here\n",
    "data.to_parquet('data_preprocessed/preprocessing_1-data.parquet', index = False)\n",
    "# data = pd.read_parquet('data_preprocessed/preprocessing_1-data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pickup and dropoff time\n",
    "\n",
    "First, we convert the pickup and dropoff time to a python pandas specific datetime format. With the datetime format, we can add details like hour, weekday or monthas.\n",
    "In the following, we calculate and assign new columns to the dataframe such as weekday,\n",
    "month and pickup_hour which will help us to gain more insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the type of each variable\n",
    "data.dtypes.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional columns with time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data.pickup_datetime.dt.month\n",
    "data['weekday'] = data.pickup_datetime.dt.weekday_name\n",
    "data['weekday_num'] = data.pickup_datetime.dt.weekday\n",
    "data['pickup_hour'] = data.pickup_datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "sns.countplot(data.month)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "plt.figure(figsize = (20,5))\n",
    "sns.countplot(data.weekday, order = weekdays)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "sns.countplot(data.pickup_hour)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# n = sns.FacetGrid(data, row='weekday', row_order = weekdays, aspect=4)\n",
    "# n.map(sns.countplot, 'pickup_hour', palette = current_palette)\n",
    "# plt.show()\n",
    "# plt.savefig('./img/weekday-pickuphour-grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the plot takes about 5 minutes, so we will incude it as an image.\n",
    "<img src='./img/weekday-pickuphour-grid.png' align='center' width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Observations\n",
    "\n",
    " * Taxi pickups seem to be consistent across the week at 3 pm.\n",
    " * Taxi pickups at 7 am from Monday to Friday, but strongly reduced on weekends\n",
    " * Taxi pickups increased in the late night hours over the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,25,3)\n",
    "labels = [ \"(%s - %s]\"%(bins[i-1], bins[i]) for i in range(1, bins.shape[0]) ]\n",
    "data['Time_Window'] = pd.cut(data['pickup_hour'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "sns.countplot(data.Time_Window)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of any unforseen issues load data and continue from here\n",
    "data.to_parquet('data_preprocessed/preprocessing_2-data.parquet')\n",
    "# data = pd.read_parquet('data_preprocessed/preprocessing_2-data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rule by New York City Taxi and Limousine Commission (TLC) agency\n",
    "\n",
    "The data trip duration is the observable, we like to predict. Therefore, it is very important that we train our machine learning algorithm based on correct data trip duration in our training dataset.\n",
    "\n",
    "The rule below applies to all TLC-licensed drivers. No company is permitted to dispatch trips to drivers who have exceeded 10 hours of passenger time in any 24-hour period.\n",
    "\n",
    "Passenger time is when a rider is in your car. It does not include time spent waiting for trip requests, driving to pick up riders, or waiting for riders to get to your car. The 10 hours (36000s) limit can be reset and the driver can get back on the road after 8 hours offline.\n",
    "\n",
    "We assume an additional 2 hour time buffer because cap driver may break the 10 hour limit. In total, we assume a maximum um 12 hour (43200 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.trip_duration <= 43200]\n",
    "data = data[data.trip_duration >= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = data.trip_duration.mean()\n",
    "median = data.trip_duration.median()\n",
    "\n",
    "fig = plt.figure(figsize = (20,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.boxplot(data.trip_duration, ax=ax)\n",
    "ax.axvline(mean, color='r', linestyle='--')\n",
    "ax.axvline(median, color='g', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trip_duration_min'] = (data.trip_duration/60.0)\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "data.trip_duration_min.groupby(pd.cut(data.trip_duration_min, np.arange(0,120,10))).count().plot(kind='bar')\n",
    "\n",
    "plt.xlabel('Trip Duration (minutes)')\n",
    "plt.ylabel('Trip Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of any unforseen issues load data and continue from here\n",
    "data.to_parquet('data_preprocessed/preprocessing_3-data.parquet')\n",
    "# data = pd.read_parquet('data_preprocessed/preprocessing_3-data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pickup and drop off coordinates\n",
    "\n",
    "We set the limits of the longitudal and latitudal coordinates of New York sufficiently to allow for trips from and to the neighbouring states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min = 40.5\n",
    "lat_max = 40.9\n",
    "lon_min = -74.2\n",
    "lon_max = -73.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['pickup_latitude'].between(lat_min, lat_max)]\n",
    "data = data[data['dropoff_latitude'].between(lat_min, lat_max)]\n",
    "\n",
    "data = data[data['pickup_longitude'].between(lon_min, lon_max)]\n",
    "data = data[data['dropoff_longitude'].between(lon_min, lon_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize = (20,5))\n",
    "ax_lati = fig.add_subplot(1, 2, 1)\n",
    "ax_long = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(data[['pickup_latitude', 'dropoff_latitude']]), ax = ax_lati)\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(data[['pickup_longitude', 'dropoff_longitude']]), ax = ax_long)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# current_palette = sns.color_palette()\n",
    "# fig = plt.figure(figsize = (20,20))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# df = data.sample(n=7200000)\n",
    "\n",
    "# pick_long= df.loc[:,'pickup_longitude'].values\n",
    "# pick_lat = df.loc[:,'pickup_latitude'].values\n",
    "\n",
    "# drop_long= df.loc[:,'dropoff_longitude'].values\n",
    "# drop_lat = df.loc[:,'dropoff_latitude'].values\n",
    "\n",
    "# ax.scatter(x=pick_long, y=pick_lat, c=np.array([current_palette[2]]), alpha=0.6, s=3, label='pick up coordinate')\n",
    "# ax.scatter(x=drop_long, y=drop_lat, c=np.array([current_palette[0]]), alpha=0.6, s=3, label='drop off coordinate')\n",
    "\n",
    "# ax.legend()\n",
    "# ax.set_xlabel('Longitude')\n",
    "# ax.set_ylabel('Latitude')\n",
    "# plt.savefig('./img/scatter_location.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the plot takes about 10 minutes, so we will incude it as an image.\n",
    "<img src='./img/scatter_location.png' align='center' width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of any unforseen issues load data and continue from here\n",
    "data.to_parquet('data_preprocessed/preprocessing_4-data.parquet')\n",
    "# data = pd.read_parquet('data_preprocessed/preprocessing_4-data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distance and speed\n",
    "\n",
    "From the starting and destination, we make a rough estimation the distance and speed.\n",
    "\n",
    "### Estimating the distance\n",
    "\n",
    "We calculated the distance (calc_distance) between the pickup and the dropoff coordinates using [Haversine formula](\n",
    "https://en.wikipedia.org/wiki/Haversine_formula). We did it in advance for you to save us time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(df):\n",
    "    from haversine import haversine\n",
    "    pickup = (df['pickup_latitude'], df['pickup_longitude'])\n",
    "    drop = (df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    return haversine(pickup, drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate an estimation of the distance and assign new column to the dataframe.\n",
    "We have done it for you and provided the column in the data set, however if you would like to work on the full TLC data set, you have the code snipplet to do it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['distance'] = data.apply(lambda x: calc_distance(x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the driving speed\n",
    "\n",
    "Speed is a function of distance and time. In our training data set we can calculate an estimation of the speed in km/h. This is a data cleaning step, because to calculate the driving speed we need the trip duration and trip duration is the unknown variable we will predict with our machine learning algorithm.\n",
    "\n",
    "In New York the maximum speed limit is 25 mph in urban area (approx. 40 km/h) and 65 mph on controlled state highways (approx 104 km/h). We assume that the average speed of a trip is below the maximum speed limit of 104 km/h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speed'] = (data.distance/(data.trip_duration/3600.0))\n",
    "data = data[data.speed <= 104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "data.speed.groupby(pd.cut(data.speed, np.arange(0,104,10))).count().plot(kind = 'barh', ax=ax)\n",
    "ax.set_xlabel('Trip count')\n",
    "ax.set_ylabel('estimated Speed (km/h)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have carefully cleaned the data, before we proceed we store the data to a [parquet](https://parquet.apache.org/) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('data_preprocessed/cleaned_data.parquet', index = False)\n",
    "# data = pd.read_parquet('data_preprocessed/cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "***\n",
    "\n",
    "After looking at the dataset from different perspectives. Let's prepare our dataset before training our model. Since our dataset do not contain very large number of dimensions. We will use the Principal Component Analysis (PCA) feature extraction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/DataScienceDevCycle_3.png' align='center' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign the values to X & y array from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.loc[:,'trip_duration'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.drop(['pickup_datetime',\n",
    "                        'dropoff_datetime',\n",
    "                        'total_amount',\n",
    "                        'trip_duration',\n",
    "                        'distance',\n",
    "                        'weekday_num',\n",
    "                        'trip_duration_min',\n",
    "                        'pickup_hour',\n",
    "                        'speed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 0\n",
    "#First check the index of the features and label\n",
    "list(zip( range(0,len(data_train.columns)),data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming categorical data\n",
    "\n",
    "Transform categorical data into a Boolean variable or in other words we dummify all the categorical features.\n",
    "\n",
    "Categorical data refers to data that represent categories - data with a fixed and unordered number of values. For example in the New York taxi data set, the columns vendor_id or month are categorical. Categorical data is a challenge for developing machine learning models.  Working with categorical data requires a transformation from a categorical to a numerical representation. A typical method is the introduction of so-called dummy variables. Dummy variables represent the presence or absence of categorical values by values 1 or 0. \n",
    "\n",
    "However, you have to ensure that the originally independent variables do not accidentally become multicollinear. In other words, you want to avoid the scenario where two or more variables are highly correlated; in simple terms, one variable can be predicted by the others. This is also known as the dummy-trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [(\"month\",\"month\"), \n",
    "           (\"weekday\", \"weekday\"),\n",
    "           (\"passenger_count\", \"passenger\"),\n",
    "           (\"Time_Window\", \"TW\")]\n",
    "\n",
    "for feature, col_prefix in cat_col:\n",
    "\n",
    "    dummy = pd.get_dummies(data_train.loc[:,feature], prefix=col_prefix)\n",
    "    # Removing the first column avoids the dummy-trap\n",
    "    dummy.drop(dummy.columns[0], axis=1, inplace=True)\n",
    "    data_train = pd.concat([data_train,dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(['month',\n",
    "                        \"weekday\",\n",
    "                        \"passenger_count\",\n",
    "                        \"Time_Window\"], axis=1)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First check the index of the features and label\n",
    "list(zip( range(0,len(data_train.columns)),data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "Before training our model on the dataset, we need to split the dataset into training and testing datasets. This is required to train our model on the major part of our dataset and test the accuracy of the model on the minor part. Let's split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split raw data\n",
    "X, X_test, y, y_test = train_test_split(X,y, random_state=191012, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "***\n",
    "We will use PCA for feature extraction i.e. Principal Component Analysis. It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "### Scale Data\n",
    "It is suggested to scale the input varibles first before applying PCA to standardise the variance and avoid the bias. Lets Scale the data using StandardScaler. The effect and importance of scaling your data is nicely demonstrated by the [scikit-learn comunity](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# corr = pd.DataFrame(X[:,:]).corr()\n",
    "# corr.index = pd.DataFrame(X[:,:]).columns\n",
    "# sns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\n",
    "# plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "# plt.savefig('./img/correlation_matrix.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix shows the correlation coefficients between two variables. Each cell represents the correlation between two variables. A correlation matrix is used as a way to summarize data, to identified correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/correlation_matrix.png' align='center' width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA application\n",
    "\n",
    "Let's apply PCA technique on the training features to understand how many principal components should we select for our model to capture atleast 90% variance. For that we will take help of plot and cumsum function of numpy package.\n",
    "\n",
    "A visual explenation of the [Principal Component Analysis](http://setosa.io/ev/principal-component-analysis/) was published by [Victor Powell](https://twitter.com/vicapow) and [Lewis Lehe](http://lewislehe.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "pca = PCA().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is applied on the training and the test dataset. Our input features are now ready for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"Method\", \"PCA\")\n",
    "run.log(\"explained_variance_ratio\", pca.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_32 = PCA(n_components=32)\n",
    "run.log(\"n_components\", 32)\n",
    "X_train_pca = pca_32.fit_transform(X)\n",
    "X_test_pca = pca_32.transform(X_test)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# corr = pd.DataFrame(X_train_pca).corr()\n",
    "# corr.index = pd.DataFrame(X_train_pca).columns\n",
    "# sns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\n",
    "# plt.title(\"Correlation Heatmap after PCA\", fontsize=16)\n",
    "# plt.savefig('./img/correlation_matrix_pca.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "All of the features shows **NO** correlation at all. Because feature extraction removes all collinearity.\n",
    " \n",
    "<img src='./img/correlation_matrix_pca.png' align='center' width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the Model now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "***\n",
    "Our machine learning shall output the duration of a taxi in New York City given the pickup time and location, tha dropoff location and number of pasengers. Since our dependent variable is the trip duration. It is a continous value so we will use regression technique to predict our output. Our independent variables are the passenger count, pickup longitude and latitude, the dropoff longitude and latitude, the month, weekday and a two hour time window.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "It is used to explain the relationship between one continuous dependent variable and two or more independent variables. Linear Regression is most likely the simplest machine learning algorithm. A detailed explanation can be found in the [Blog article](https://towardsdatascience.com/linear-regression-understanding-the-theory-7e53ac2831b5) by [Marco Peixeiro](https://towardsdatascience.com/@marcopeixeiro) Linear regression assumes a linear relation between the input variable (independent variables) and the predicted outcome (dependent variable). Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). By construction the linear regression is not sensitive towards scaling the independent input variables. Let's proceed\n",
    "\n",
    "### Model training\n",
    "\n",
    "We first will use the transformed (PCA) and scaled input data. Training a linear regrission model is quite fast even on very large datasets. So considering the size of our dataset this seems to be the correct approach as of now. Let's see how it performs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/DataScienceDevCycle_4.png' align='center' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train Machine Learning Model: Linear Regression with scaling & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "run.log(\"Method\", \"Linear Regression with PCA\")\n",
    "run.log(\"Dataset Size\", X_train_pca.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Linear regressor for the Feature extraction group with pca\n",
    "regressor = LinearRegression() \n",
    "regressor.fit(X_train_pca,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the machine learning model\n",
    "***\n",
    "\n",
    "Our model predicits a continues variable, namely the NYC taxi trip duration. To estimate the accuracy of our machine learning model, we will use one of the most popular evaluation metric used in regression problems -- Root Mean Squared Error (RMSE). The assumption behind the RMSE is that error are unbiased and follow a normal distribution. The reliability off the RMSE as a evaluation metric depends strongly on the fullfilment of this asssumptions.\n",
    "\n",
    "$$ RMSE = \\sqrt{ \\frac{1}{N} \\sum_{i}^{N} (\\text{predicted}_i - \\text{actual}_i)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $N$ is the total number of entries in our test dataset (total number of observations). For every entry $i$ the difference between the predicted value, in our case the duration of a taxi trip, and the actual value is calculated. The RMSE is particularly robust due to its ‘squared’ nature which prevents cancelling the positive and negative error values.\n",
    "When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.\n",
    "RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.\n",
    "As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_5.png' align='center' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluate Machine Learning Model: Linear Regression with scaling & PCA\n",
    "\n",
    "We can write down the linear regression formula:\n",
    "$$ y_i = \\beta_{0} + \\beta_1 \\cdot x_{i1} + \\beta_2 \\cdot x_{i2} + \\beta_3 \\cdot x_{i3} + ... + \\beta_{31} \\cdot x_{i32} + \\varepsilon = \\beta_0 + \\sum_j^{p=32} \\beta_j \\cdot x_{ij}  $$\n",
    "\n",
    "The parameter $\\beta$ have been determined by our machine learning training using a [least squares fitting](http://mathworld.wolfram.com/LeastSquaresFitting.html) algorithm.\n",
    "We can output the parameter $\\beta_{0}$ which represents the y-axis intersection and the 32 $\\beta_{1...32}$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Predict from the test features of Feature Extraction group\n",
    "y_pred = regressor.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "sigma =regressor.score(X_test_pca, y_test)\n",
    "run.log(\"RMSE\", RMSE)\n",
    "run.log(\"variance\", sigma)\n",
    "run.log(\"Time train [s]\", 31.5)\n",
    "run.log(\"Time inference [s]\", 0.16)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE score for the Multiple LR PCA is : {0:.2f} seconds'.format(RMSE))\n",
    "print('Variance score for the Multiple LR PCA is : %.2f' % sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance score in the context of a linear regression, measure of how far observed values differ from the average of predicted values, i.e., their difference from the predicted value mean. The goal is to have a value that is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Machine Learning Model: Linear Regression without scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_4.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "run.log(\"Method\", \"Linear Regression\")\n",
    "run.log(\"Dataset Size\", X_train_pca.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Linear regressor for the raw data\n",
    "regressor = LinearRegression() \n",
    "regressor.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate Machine Learning Model: Linear Regression without scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_5.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Predict from the test features of raw data\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "sigma =regressor.score(X_test, y_test)\n",
    "run.log(\"RMSE\", RMSE)\n",
    "run.log(\"variance\", sigma)\n",
    "run.log(\"Time train [s]\", 32.8)\n",
    "run.log(\"Time inference [s]\", 0.148)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE score for the Multiple LR: {0:.2f} seconds'.format(RMSE))\n",
    "print('Variance score for the Multiple LR: %.2f' % sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Machine Learning Model: Random Forest Regression with scaling & PCA\n",
    "\n",
    "A [random forest](https://en.wikipedia.org/wiki/Random_forest) regressor is a collection or smartly combined collection of [decision trees](https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d) (see the blog article by [Frank Ceballos](https://towardsdatascience.com/@frank.ceballos)). One can see a random forest as a meta estimator that fits a number of decision trees on various sub-samples of the dataset and uses for example averaging to improve the predictive accuracy.\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_4.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(X_train_pca.shape[0], size=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "run.log(\"Method\", \"RandomForestRegressor\")\n",
    "run.log(\"Dataset Size\", 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#instantiate the object for the Random Forest Regressor with scaling & PCA\n",
    "regressor = RandomForestRegressor(n_jobs=-1)\n",
    "regressor.fit(X_train_pca[idx,:],y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Machine Learning Model: Random Forest Regression with scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_5.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in regressor.get_params():\n",
    "    run.log(k, regressor.get_params()[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Predict the output with object of PCA params for Feature Extraction Group\n",
    "y_pred = regressor.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "sigma =regressor.score(X_test_pca, y_test)\n",
    "run.log(\"RMSE\", RMSE)\n",
    "run.log(\"variance\", sigma)\n",
    "run.log(\"Time train [s]\", 746)\n",
    "run.log(\"Time inference [s]\", 41.2)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE score for the Multiple LR PCA is : {0:.2f} seconds'.format(RMSE))\n",
    "print('Variance score for the Multiple LR PCA is : %.2f' % sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Machine Learning Model: Random Forest Regression without scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_4.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "run.log(\"Method\", \"RandomForestRegressor without scaling & PCA\")\n",
    "run.log(\"Dataset Size\", 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#instantiate the object for the Random Forest Regressor\n",
    "regressor = RandomForestRegressor(n_jobs=-1)\n",
    "regressor.fit(X[idx,:],y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in regressor.get_params():\n",
    "    run.log(k, regressor.get_params()[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Machine Learning Model: Random Forest Regression without scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_5.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Predict the output with object of default params for Feature Selection Group\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "sigma = regressor.score(X_test, y_test)\n",
    "run.log(\"RMSE\", RMSE)\n",
    "run.log(\"variance\", sigma)\n",
    "run.log(\"Time train [s]\", 236)\n",
    "run.log(\"Time inference [s]\", 45.8)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the raw data\n",
    "print('RMSE score for the Multiple LR raw is : {0:.2f} seconds'.format(RMSE))\n",
    "print('Variance score for the Multiple LR raw is : %.2f' %sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Machine Learning Model: Random Forest Regression with scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_4.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  experiment.start_logging()\n",
    "run.log(\"Method\", \"RandomForestRegressor\")\n",
    "run.log(\"Dataset Size\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regressor = RandomForestRegressor(n_estimators = 26,\n",
    "                                  max_depth = 24,\n",
    "                                  min_samples_split = 9,\n",
    "                                  n_jobs=-1)\n",
    "regressor.fit(X_train_pca[idx,:],y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in regressor.get_params():\n",
    "    run.log(k, regressor.get_params()[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Machine Learning Model: Random Forest Regression with scaling & PCA\n",
    "\n",
    "<img src='./img/DataScienceDevCycle_5.png' align='left' width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = regressor.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "sigma =regressor.score(X_test_pca, y_test)\n",
    "run.log(\"RMSE\", RMSE)\n",
    "run.log(\"variance\", sigma)\n",
    "run.log(\"Time train [s]\", 71)\n",
    "run.log(\"Time inference [s]\", 25.2)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regressor on the Feature extraction group\n",
    "print('RMSE score for the Multiple LR PCA is : {0:.2f} seconds'.format(RMSE))\n",
    "print('Variance score for the Multiple LR PCA is : %.2f' % sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
